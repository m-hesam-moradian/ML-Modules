{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cbd502f",
   "metadata": {},
   "source": [
    "# ML Pipeline Test - Multiple Models Comparison\n",
    "This notebook demonstrates the usage of our ML pipeline module with multiple models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adec9ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.main import run_model_pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import PatternFill, Alignment, Font\n",
    "from src.Exel_Modules.excel_utils import save_predictions_and_metrics_to_excel\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "865c2921",
   "metadata": {},
   "source": [
    "## Define Models\n",
    "We'll test multiple models with different configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7ee7e4",
   "metadata": {},
   "source": [
    "# Define models to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa98e70f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "models = {\n",
    "    'Random Forest': {\n",
    "        'library': 'sklearn.ensemble',\n",
    "        'function': 'RandomForestClassifier',\n",
    "        'params': {'n_estimators': 100, 'max_depth': 10}\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'library': 'sklearn.ensemble',\n",
    "        'function': 'GradientBoostingClassifier',\n",
    "        'params': {'n_estimators': 100, 'learning_rate': 0.1}\n",
    "    },\n",
    "    'SVM': {\n",
    "        'library': 'sklearn.svm',\n",
    "        'function': 'SVC',\n",
    "        'params': {'kernel': 'rbf', 'C': 1.0}\n",
    "    },\n",
    "    'Logistic Regression': {\n",
    "        'library': 'sklearn.linear_model',\n",
    "        'function': 'LogisticRegression',\n",
    "        'params': {'max_iter': 1000}\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4be297d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Random Forest...\n",
      "[Train] Accuracy: 0.9683 | Precision: 0.9702 | Recall: 0.9659 | f1_score: 0.9680\n",
      "[Test ] Accuracy: 0.4712 | Precision: 0.4560 | Recall: 0.4500 | f1_score: 0.4530\n",
      "[All  ] Accuracy: 0.8688 | Precision: 0.8698 | Recall: 0.8644 | f1_score: 0.8671\n",
      "\n",
      "Training Gradient Boosting...\n",
      "[Train] Accuracy: 0.7039 | Precision: 0.7113 | Recall: 0.6811 | f1_score: 0.6959\n",
      "[Test ] Accuracy: 0.5083 | Precision: 0.4945 | Recall: 0.4737 | f1_score: 0.4839\n",
      "[All  ] Accuracy: 0.6648 | Precision: 0.6686 | Recall: 0.6403 | f1_score: 0.6542\n",
      "\n",
      "Training SVM...\n",
      "[Train] Accuracy: 0.6302 | Precision: 0.6376 | Recall: 0.5941 | f1_score: 0.6151\n",
      "[Test ] Accuracy: 0.4930 | Precision: 0.4781 | Recall: 0.4605 | f1_score: 0.4692\n",
      "[All  ] Accuracy: 0.6028 | Precision: 0.6054 | Recall: 0.5678 | f1_score: 0.5860\n",
      "\n",
      "Training Logistic Regression...\n",
      "[Train] Accuracy: 0.5309 | Precision: 0.5299 | Recall: 0.5019 | f1_score: 0.5156\n",
      "[Test ] Accuracy: 0.5122 | Precision: 0.4986 | Recall: 0.4684 | f1_score: 0.4830\n",
      "[All  ] Accuracy: 0.5272 | Precision: 0.5238 | Recall: 0.4953 | f1_score: 0.5092\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Data parameters\n",
    "DATA_PATH = \"data/FraudDetectionDataset.xlsx\"\n",
    "TARGET_COLUMN = \"Fraudulent\"\n",
    "\n",
    "# Store results for all models\n",
    "all_results = {}\n",
    "\n",
    "# Run pipeline for each model\n",
    "for model_name, model_config in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    results = run_model_pipeline(\n",
    "        data_path=DATA_PATH,\n",
    "        target_column=TARGET_COLUMN,\n",
    "        model_library=model_config['library'],\n",
    "        model_function=model_config['function'],\n",
    "        model_params=model_config['params']\n",
    "    )\n",
    "    all_results[model_name] = results\n",
    "# print(all_results[\"Random Forest\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de279bf",
   "metadata": {},
   "source": [
    "# ==== Save Predictions to Excel ===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d00b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Call the function\n",
    "save_predictions_and_metrics_to_excel(\n",
    "    results_dict=all_results,\n",
    "    model_name=\"Random Forest\",\n",
    "    excel_path=\"data/FraudDetectionDataset.xlsx\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "631a54f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your Excel path\n",
    "DATA_PATH = \"data/FraudDetectionDataset.xlsx\"\n",
    "\n",
    "# Get everything from all_results\n",
    "result_all = all_results[\"Random Forest\"][\"all\"]\n",
    "n_train = len(all_results[\"Random Forest\"][\"train\"][\"true_values\"])\n",
    "\n",
    "# Extract true and predicted values\n",
    "all_true_values = result_all[\"true_values\"]\n",
    "all_predictions = result_all[\"predictions\"]\n",
    "\n",
    "# Separate into train and test labels\n",
    "data_labels = [\"train\"] * n_train + [\"test\"] * (len(all_true_values) - n_train)\n",
    "\n",
    "# Extract metrics from the same dict (excluding lists)\n",
    "metrics = {k: v for k, v in result_all.items() if k not in [\"predictions\", \"true_values\"]}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af1e136",
   "metadata": {},
   "source": [
    "# === 1. Create dataframe ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "44ff8f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined = pd.DataFrame({\n",
    "    \"ID\": range(1, len(all_true_values) + 1),\n",
    "    \"True\": all_true_values,\n",
    "    \"Predicted\": all_predictions,\n",
    "    \"Set\": data_labels\n",
    "})\n",
    "\n",
    "# Save prediction table (without Set column)\n",
    "with pd.ExcelWriter(DATA_PATH, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    df_combined.drop(columns=[\"Set\"]).to_excel(writer, sheet_name=\"RF\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cef4d03e",
   "metadata": {},
   "source": [
    "# === 2. Styling and metric table ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "1666553f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "wb = load_workbook(DATA_PATH)\n",
    "ws = wb[\"RF\"]\n",
    "\n",
    "# Styling\n",
    "train_fill = PatternFill(start_color=\"DFF0D8\", end_color=\"DFF0D8\", fill_type=\"solid\")\n",
    "test_fill = PatternFill(start_color=\"F2DEDE\", end_color=\"F2DEDE\", fill_type=\"solid\")\n",
    "center_align = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "bold_font = Font(bold=True)\n",
    "\n",
    "# Get column index of \"Predicted\"\n",
    "header = [cell.value for cell in ws[1]]\n",
    "predicted_col_index = header.index(\"Predicted\") + 1\n",
    "\n",
    "# Apply styles and color only to Predicted column\n",
    "for row_idx, row in enumerate(ws.iter_rows(min_row=2, max_row=ws.max_row), start=0):\n",
    "    for col_idx, cell in enumerate(row, start=1):\n",
    "        cell.alignment = center_align\n",
    "        if col_idx == predicted_col_index:\n",
    "            fill = train_fill if data_labels[row_idx] == \"train\" else test_fill\n",
    "            cell.fill = fill\n",
    "\n",
    "# Style header\n",
    "for cell in ws[1]:\n",
    "    cell.alignment = center_align\n",
    "    cell.font = bold_font\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cfc043",
   "metadata": {},
   "source": [
    "# === 3. Add metrics table to the right of predictions ===\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "65048e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openpyxl.styles import PatternFill, Alignment, Font\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "def write_metrics_table(ws, metrics_dict, start_row, start_col, fill_color, label):\n",
    "    center_align = Alignment(horizontal=\"center\", vertical=\"center\")\n",
    "    bold_font = Font(bold=True)\n",
    "    fill = PatternFill(start_color=fill_color, end_color=fill_color, fill_type=\"solid\")\n",
    "\n",
    "    # Write label merged across the number of metrics columns\n",
    "    n_cols = len(metrics_dict)\n",
    "    end_col_letter = get_column_letter(start_col + n_cols - 1)\n",
    "    ws.merge_cells(start_row=start_row, start_column=start_col, end_row=start_row, end_column=start_col + n_cols - 1)\n",
    "    label_cell = ws.cell(row=start_row, column=start_col, value=label)\n",
    "    label_cell.font = Font(bold=True, color=\"FFFFFF\")\n",
    "    label_cell.alignment = center_align\n",
    "    label_cell.fill = fill\n",
    "\n",
    "    # Write headers below label\n",
    "    header_row = start_row + 1\n",
    "    for i, header in enumerate(metrics_dict.keys()):\n",
    "        cell = ws.cell(row=header_row, column=start_col + i, value=header.capitalize())\n",
    "        cell.font = bold_font\n",
    "        cell.alignment = center_align\n",
    "        cell.fill = fill\n",
    "\n",
    "    # Write values below headers\n",
    "    value_row = header_row + 1\n",
    "    for i, value in enumerate(metrics_dict.values()):\n",
    "        cell = ws.cell(row=value_row, column=start_col + i, value=round(value, 4))\n",
    "        cell.alignment = center_align\n",
    "\n",
    "# Usage example:\n",
    "\n",
    "metrics_all = all_results[\"Random Forest\"][\"all\"]\n",
    "metrics_train = all_results[\"Random Forest\"][\"train\"]\n",
    "metrics_test = all_results[\"Random Forest\"][\"test\"]\n",
    "\n",
    "start_col = ws.max_column + 1\n",
    "start_row = 1\n",
    "\n",
    "write_metrics_table(ws, \n",
    "                    {k: v for k, v in metrics_all.items() if k not in [\"predictions\", \"true_values\"]}, \n",
    "                    start_row, start_col, \"ADD8E6\", \"All Metrics\")\n",
    "\n",
    "write_metrics_table(ws, \n",
    "                    {k: v for k, v in metrics_train.items() if k not in [\"predictions\", \"true_values\"]}, \n",
    "                    start_row + 5, start_col, \"90EE90\", \"Train Metrics\")\n",
    "\n",
    "write_metrics_table(ws, \n",
    "                    {k: v for k, v in metrics_test.items() if k not in [\"predictions\", \"true_values\"]}, \n",
    "                    start_row + 10, start_col, \"FFC0CB\", \"Test Metrics\")\n",
    "\n",
    "\n",
    "wb.save(DATA_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
